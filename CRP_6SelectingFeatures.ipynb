{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"CRP_6SelectingFeatures.ipynb","provenance":[],"collapsed_sections":["OjPE95vCSh8C","9uABH8xxSh8c","lJywS3ZeSh8s","BU49vF-USh8t","jtnL3-GkSh89","4IsoN1IfSh9G","RRYcGm0fSh9i","psQ9CH1KSh9j","DwZ9t3EnSh-C","jRBpd8XvSh-I","1tuDAJ-9Sh-P","OeabSN1ESh-Q","_uiYQZ33Sh-t","FWlKYuaVSh_M"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"G4g5-aOJSh7B","colab_type":"text"},"source":["# Seleccionando Características para mejorar el desempeño\n","Este Laboratorio es Basado en el Curso de Dimensionality Reduction de DATACAMP®\n","\n","\n","En este laboratorio implementaremos técnicas para granatizar el desempeño de nuestro modelo de reconocimmiento de patrones, apicando técnicas de selección de características.\n"]},{"cell_type":"markdown","metadata":{"id":"g0uqkTofSh7D","colab_type":"text"},"source":["# Contruyendo un clasificador para detectar diabetes\n","\n","En este laboratorio se utilizará el conjunto de datos sobre diabetes de la tribu indigena Pima para predecir si una persona tiene diabetes mediante regresión logística. Hay 8 características y un objetivo (Y) en este conjunto de datos. Los datos se han dividido en un conjunto de entrenamiento y otro de prueba. A continuación se cargarán como  X_train, y_train, X_test, y y_test."]},{"cell_type":"code","metadata":{"id":"YutmrZspSh7F","colab_type":"code","colab":{},"outputId":"3c8f7fc4-acc5-4d17-819a-42470f45e029"},"source":["# import required libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","X_train=pd.read_csv(\"Pima_X_train.csv\")#reading a dataset in a dataframe using pandas\n","print(\"X_train\",X_train.shape)\n","\n","X_test=pd.read_csv(\"Pima_X_test.csv\")#reading a dataset in a dataframe using pandas\n","print(\"X_test\",X_test.shape)\n","\n","y_train = X_train['result']\n","print(\"y_train\",y_train.shape)\n","\n","\n","y_test = X_test['result']\n","print(\"y_test\",y_train.shape)\n","\n","X_train = X_train.drop('result', axis=1)\n","print(\"X_train droped\",X_train.shape)\n","\n","X_test = X_test.drop('result', axis=1)\n","print(\"X_test droped\",X_test.shape)\n","\n","#X=[X_train,X_test]\n","X = pd.concat([X_train, X_test], axis=0)\n","print(\"X\",X.shape)\n","\n","y=pd.concat([y_train, y_test], axis=0)\n","print(\"y\",y.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train (294, 9)\n","X_test (98, 9)\n","y_train (294,)\n","y_test (294,)\n","X_train droped (294, 8)\n","X_test droped (98, 8)\n","X (392, 8)\n","y (392,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U4_CE-kgSh7P","colab_type":"text"},"source":["Se carga una instancia  StandardScaler() predefinida como scaler una función de regresión logística LogisticRegression() definida como lr."]},{"cell_type":"code","metadata":{"id":"g5WvW6vcSh7R","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","lr = LogisticRegression(solver='liblinear')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKe044UkSh7X","colab_type":"text"},"source":["Ajuste el escalador en las características de entrenamiento y transforme estas características en una única  en un sólo comando."]},{"cell_type":"code","metadata":{"id":"7Exh8Fy3Sh7Z","colab_type":"code","colab":{}},"source":["# Fit the scaler on the training features and transform these in one go\n","X_train_std = scaler.____(____)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gst_X3W0Sh7f","colab_type":"text"},"source":["Ajuste el modelo de regresión logística en los datos de entrenamiento escalados."]},{"cell_type":"code","metadata":{"id":"WwjJIg1RSh7g","colab_type":"code","colab":{},"outputId":"3fd91b69-cfab-4d84-fb8e-1c28534563a5"},"source":["# Fit the logistic regression model on the scaled training data\n","lr.____(____, ____)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='warn', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":139}]},{"cell_type":"markdown","metadata":{"id":"J6Tj7PlqSh7n","colab_type":"raw"},"source":["Escalar las características del conjunto test."]},{"cell_type":"code","metadata":{"id":"pP3lpGC6Sh7o","colab_type":"code","colab":{}},"source":["# Scale the test features\n","X_test_std = scaler.____(____)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzETs6xWSh7u","colab_type":"text"},"source":["Predecir la presencia de diabetes en el conjunto de pruebas que fueron escaladas."]},{"cell_type":"code","metadata":{"id":"VHqLUMxsSh7v","colab_type":"code","colab":{}},"source":["# Predict diabetes presence on the scaled test set\n","y_pred = lr.____(____)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdWEoVMZSh76","colab_type":"text"},"source":["   Imprimir los resultados de precision"]},{"cell_type":"code","metadata":{"id":"SMwasc3ASh78","colab_type":"code","colab":{},"outputId":"36cdea6a-dd8d-40c2-e3e0-8f90d4e01293"},"source":["# Prints accuracy metrics and feature coefficients\n","print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["79.6% accuracy on test set.\n","{'pregnant': 0.04, 'glucose': 1.23, 'diastolic': 0.03, 'triceps': 0.24, 'insulin': 0.19, 'bmi': 0.38, 'family': 0.34, 'age': 0.34}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kFdaglqwSh8B","colab_type":"text"},"source":["¡Excelente! Hemos obtenido casi un 80% de precisión en el conjunto de prueba. Eche un vistazo a las diferencias en los coeficientes del modelo para las diferentes características."]},{"cell_type":"markdown","metadata":{"id":"OjPE95vCSh8C","colab_type":"text"},"source":["# Eliminación de características de manera manual y recursiva\n","\n","Ahora que hemos creado un clasificador de diabetes, veamos si podemos reducir la cantidad de características sin dañar demasiado la precisión del modelo.\n","\n","En la segunda línea de código de éste bloque, las características han sido seleccionadas  del dataframe original. Realice los ajustes pertinentes.\n"]},{"cell_type":"markdown","metadata":{"id":"dCsHiOHySh8D","colab_type":"text"},"source":["Ejecute el código dado, luego elimine la característica de X con el coeficiente más bajo de acuerdo a lo generado por el modelo. "]},{"cell_type":"code","metadata":{"id":"M5GwNAbySh8E","colab_type":"code","colab":{},"outputId":"6f1e830b-9466-47a9-f31b-a21d8f21b4be"},"source":["from sklearn.model_selection import train_test_split\n","\n","diabetes_df=X;\n","\n","# Remove the feature with the lowest model coefficient\n","X = diabetes_df[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n","\n","# Performs a 25-75% train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=________, random_state=0)\n","\n","# Scales features and fits the logistic regression model\n","lr.fit(scaler.fit_transform(X_train), ______)\n","\n","# Calculates the accuracy on the test set and prints coefficients\n","acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) \n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["77.6% accuracy on test set.\n","{'pregnant': 0.34, 'glucose': 1.16, 'triceps': 0.23, 'insulin': 0.06, 'bmi': 0.31, 'family': 0.48, 'age': 0.31}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9-ADory7Sh8K","colab_type":"text"},"source":["Ejecute el código y elimine 2 características más con los coeficientes más bajos del modelo."]},{"cell_type":"code","metadata":{"id":"XQ1bgHasSh8L","colab_type":"code","colab":{},"outputId":"081477af-472b-42e7-9dde-518501c1661f"},"source":["# Remove the 2 features with the lowest model coefficients\n","X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n","\n","# Performs a 25-75% train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","# Scales features and fits the logistic regression model\n","lr.fit(scaler.fit_transform(X_train), y_train)\n","\n","# Calculates the accuracy on the test set and prints coefficients\n","acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) \n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["78.6% accuracy on test set.\n","{'pregnant': 0.35, 'glucose': 1.19, 'bmi': 0.46, 'family': 0.49, 'age': 0.32}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UtmvZ30wSh8T","colab_type":"text"},"source":["Ejecute el código y solo mantenga la caracterísitca con el coeficiente más alto."]},{"cell_type":"code","metadata":{"id":"QCcQiVIoSh8U","colab_type":"code","colab":{},"outputId":"92eab32f-ca4f-48f3-9710-8484249e139f"},"source":["# Only keep the feature with the highest coefficient\n","X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n","\n","# Performs a 25-75% train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","# Scales features and fits the logistic regression model to the data\n","lr.fit(scaler.fit_transform(X_train), y_train)\n","\n","# Calculates the accuracy on the test set and prints coefficients\n","acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) \n","print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["76.5% accuracy on test set.\n","{'glucose': 1.35}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkfDwfTTSh8b","colab_type":"text"},"source":["¡Buen trabajo! Eliminar todas las funciones menos una solo redujo la precisión en un pequeño porcentaje."]},{"cell_type":"markdown","metadata":{"id":"9uABH8xxSh8c","colab_type":"text"},"source":["# Eliminación de características de manera automática y recursiva\n","\n","Ahora automaticemos este proceso recursivo. Cree un Wrap Eliminador de características recursivas (RFE) alrededor de nuestro estimador de regresión logística y defina el número deseado de características."]},{"cell_type":"markdown","metadata":{"id":"q4Y3dooTSh8d","colab_type":"text"},"source":["Importamos nuevamente nuestors datos originales"]},{"cell_type":"code","metadata":{"id":"IA4o-3IbSh8e","colab_type":"code","colab":{},"outputId":"0aa39495-437a-48cd-8de0-80d739b29f61"},"source":["X_train=pd.read_csv(\"Pima_X_train.csv\")#reading a dataset in a dataframe using pandas\n","\n","\n","X_test=pd.read_csv(\"Pima_X_test.csv\")#reading a dataset in a dataframe using pandas\n","\n","\n","y_train = X_train['result']\n","\n","\n","\n","y_test = X_test['result']\n","\n","\n","X_train = X_train.drop('result', axis=1)\n","print(\"X_train \",X_train.shape)\n","\n","X_test = X_test.drop('result', axis=1)\n","print(\"X_test \",X_test.shape)\n","\n","#X=[X_train,X_test]\n","X = pd.concat([X_train, X_test], axis=0)\n","print(\"X\",X.shape)\n","\n","y=pd.concat([y_train, y_test], axis=0)\n","print(\"y\",y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train  (294, 8)\n","X_test  (98, 8)\n","X (392, 8)\n","y (392,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c6iUxB-DSh8j","colab_type":"text"},"source":["Cree el RFE con un LogisticRegression()estimador y seleccionando sólo 3 características."]},{"cell_type":"code","metadata":{"id":"wX0RDT8XSh8k","colab_type":"code","colab":{},"outputId":"402a78a7-fd15-487c-8b53-e76b9647b67b"},"source":["from sklearn.feature_selection import RFE\n","# Create the RFE with a LogisticRegression estimator and 3 features to select\n","rfe = ____(estimator=____, n_features_to_select=____, verbose=1)\n","\n","# Fits the eliminator to the data\n","rfe.fit(X_train, y_train)\n","\n","# Print the features and their ranking (high = dropped early on)\n","print(dict(zip(X.columns, rfe.____)))\n","\n","# Print the features that are not eliminated\n","print(X.columns[rfe.____])\n","\n","# Calculates the test set accuracy\n","acc = accuracy_score(y_test, rfe.predict(X_test))\n","print(\"{0:.1%} accuracy on test set.\".format(acc)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting estimator with 8 features.\n","Fitting estimator with 7 features.\n","Fitting estimator with 6 features.\n","Fitting estimator with 5 features.\n","Fitting estimator with 4 features.\n","{'pregnant': 1, 'glucose': 1, 'diastolic': 3, 'triceps': 2, 'insulin': 6, 'bmi': 5, 'family': 1, 'age': 4}\n","Index(['pregnant', 'glucose', 'family'], dtype='object')\n","80.6% accuracy on test set.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S5VCKCLjSh8r","colab_type":"text"},"source":["¡Excelente! Al eliminar 5 características,  definimos las 3 más relevantes, obtenemos así una precisión del 80,6% en el conjunto de prueba."]},{"cell_type":"markdown","metadata":{"id":"lJywS3ZeSh8s","colab_type":"text"},"source":["# Selección de características Utilizando Árboles de decisión\n","\n","Ahora utilizaremos Árboles de decisión que nos permitirán identificar las características candidatas a ser removidas."]},{"cell_type":"markdown","metadata":{"id":"BU49vF-USh8t","colab_type":"text"},"source":["# Construyendo un Modelo de Bósques aleatorios\n","\n","Trabajaremos igualmente con  en el conjunto de datos Pima para predecir si un individuo tiene o nó diabetes. Esta vez usando un clasificador de bosque aleatorio. Ajustará el modelo en los datos de entrenamiento después de realizar la división de prueba de tren y consultará los valores de importancia de la función.\n","\n","Ahora cargaremos las características y los conjuntos de datos de datos como X y y. Así como  los paquetes y las funciones necesarias."]},{"cell_type":"code","metadata":{"id":"buCyL4jeSh8u","colab_type":"code","colab":{},"outputId":"4ea57011-7b2d-4afa-f02f-e46400f15deb"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","#rf = RandomForestClassifier(n_estimators=10)\n","rf = RandomForestClassifier()\n","\n","X_train=pd.read_csv(\"Pima_X_train.csv\")#reading a dataset in a dataframe using pandas\n","\n","\n","X_test=pd.read_csv(\"Pima_X_test.csv\")#reading a dataset in a dataframe using pandas\n","\n","\n","y_train = X_train['result']\n","\n","\n","\n","y_test = X_test['result']\n","\n","\n","X_train = X_train.drop('result', axis=1)\n","print(\"X_train \",X_train.shape)\n","\n","X_test = X_test.drop('result', axis=1)\n","print(\"X_test \",X_test.shape)\n","\n","#X=[X_train,X_test]\n","X = pd.concat([X_train, X_test], axis=0)\n","print(\"X\",X.shape)\n","\n","y=pd.concat([y_train, y_test], axis=0)\n","print(\"y\",y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train  (294, 8)\n","X_test  (98, 8)\n","X (392, 8)\n","y (392,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OE_GWGl7Sh8z","colab_type":"text"},"source":["Establezca un conjunto de evaluación del 25%,  estableciendo una relación de conjuntos de entrenamiento-prueba 75% -25%."]},{"cell_type":"code","metadata":{"id":"R8WE6hNESh81","colab_type":"code","colab":{},"outputId":"dfdaaa18-5de8-4ec7-ab8a-396557d1dccf"},"source":["# Perform a 75% training and 25% test data split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=____, random_state=0)\n","\n","# Fit the random forest model to the training data\n","rf = RandomForestClassifier(random_state=0)\n","rf.____(____, ____)\n","\n","# Calculate the accuracy\n","acc = accuracy_score(____, ____)\n","\n","# Print the importances per feature\n","print(dict(zip(X.columns, rf.____.round(2))))\n","\n","# Print accuracy\n","print(\"{0:.1%} accuracy on test set.\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'pregnant': 0.09, 'glucose': 0.19, 'diastolic': 0.06, 'triceps': 0.11, 'insulin': 0.18, 'bmi': 0.11, 'family': 0.08, 'age': 0.17}\n","78.6% accuracy on test set.\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"BOQWb69xSh88","colab_type":"text"},"source":["¡Buen trabajo! El modelo de bosques aleatorios obtiene casi un 79% de precisión en el conjunto de prueba y la \"glucosa\" es la característica más importante (0.19)."]},{"cell_type":"markdown","metadata":{"id":"jtnL3-GkSh89","colab_type":"text"},"source":["# Bosque Aleatorio para la selección de características\n","\n","Ahora usemos el modelo de Bosque aleatorio ajustado para seleccionar las características más importantes de nuestro conjunto de datos de entrada X.\n","\n","rf, corresponde al modelo entrenado en la secuencia anterior."]},{"cell_type":"code","metadata":{"id":"CFuDiFJcSh89","colab_type":"code","colab":{},"outputId":"e1dfd7ea-434d-4074-fbb7-becd6d03ca76"},"source":["# Create a mask for features importances above the threshold\n","mask = ____\n","\n","# Prints out the mask\n","print(mask)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[False  True False False  True False False  True]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e1k-sl0fSh9B","colab_type":"text"},"source":["Seleccione las características más importantes aplicando la máscara a X."]},{"cell_type":"code","metadata":{"id":"DKRVwutOSh9C","colab_type":"code","colab":{},"outputId":"bf415981-00dd-415c-9c4e-5828eba51576"},"source":["# Create a mask for features importances above the threshold\n","mask = rf.feature_importances_ > 0.15\n","\n","# Apply the mask to the feature dataset X\n","reduced_X = ____\n","\n","# prints out the selected column names\n","print(reduced_X.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['glucose', 'insulin', 'age'], dtype='object')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vINJZDHZSh9F","colab_type":"text"},"source":["¡Bien hecho! Solo las características 'glucosa' 'insulina' y 'age' se consideraron suficientemente importantes."]},{"cell_type":"markdown","metadata":{"id":"4IsoN1IfSh9G","colab_type":"text"},"source":["# Eliminación de características recursivas con bosques aleatorios\n","\n","Cree un Wrap Eliminador de características recursivas (RFE) alrededor de un modelo de bosque aleatorio para eliminar las características paso a paso. Este método es más conservador en comparación con la selección de características después de aplicar un umbral  único de importancia. Dado que descartar una característica puede influir en las importancias relativas de las otras."]},{"cell_type":"markdown","metadata":{"id":"K3cPxTHISh9G","colab_type":"text"},"source":["Cree un eliminador de características recursivas que seleccionará las 2 características más importantes utilizando un modelo de bosque aleatorio.\n"]},{"cell_type":"code","metadata":{"id":"xf4MnHHBSh9H","colab_type":"code","colab":{}},"source":["# Wrap the feature eliminator around the random forest model\n","rfe = ____(estimator=____, n_features_to_select=____, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOSTSC8VSh9P","colab_type":"text"},"source":["Ajuste el RFE a los datos de entrenamiento"]},{"cell_type":"code","metadata":{"id":"djvQBo5TSh9Q","colab_type":"code","colab":{},"outputId":"a6544a66-854a-4a50-f928-b6eec6912fdb"},"source":["# Fit the model to the training data\n","rfe.____(____, ____)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting estimator with 8 features.\n","Fitting estimator with 7 features.\n","Fitting estimator with 6 features.\n","Fitting estimator with 5 features.\n","Fitting estimator with 4 features.\n","Fitting estimator with 3 features.\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["RFE(estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n","                                     criterion='gini', max_depth=None,\n","                                     max_features='auto', max_leaf_nodes=None,\n","                                     min_impurity_decrease=0.0,\n","                                     min_impurity_split=None,\n","                                     min_samples_leaf=1, min_samples_split=2,\n","                                     min_weight_fraction_leaf=0.0,\n","                                     n_estimators='warn', n_jobs=None,\n","                                     oob_score=False, random_state=None,\n","                                     verbose=0, warm_start=False),\n","    n_features_to_select=2, step=1, verbose=1)"]},"metadata":{"tags":[]},"execution_count":164}]},{"cell_type":"markdown","metadata":{"id":"YbDuG0EeSh9T","colab_type":"text"},"source":["Cree una máscara usando el eliminador ajustado y apliquelo al dataset X"]},{"cell_type":"code","metadata":{"id":"rS_fiuyOSh9U","colab_type":"code","colab":{},"outputId":"6032d8ee-e17b-4afe-d8d6-414adbb127d1"},"source":["# Create a mask using an attribute of rfe\n","mask = ____\n","\n","# Apply the mask to the feature dataset X and print the result\n","reduced_X = ____\n","print(reduced_X.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['glucose', 'insulin'], dtype='object')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3TS4hiIESh9a","colab_type":"text"},"source":["Cambie los parámetros del RFE() para elimnar dos caracterísiticas a cada paso (step)"]},{"cell_type":"code","metadata":{"id":"IHXXX_isSh9c","colab_type":"code","colab":{},"outputId":"096cb03c-121f-4c3e-c033-9d5ad9ec5d1c"},"source":["# Set the feature eliminator to remove 2 features on each step\n","rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, ____, verbose=1)\n","\n","# Fit the model to the training data\n","rfe.fit(X_train, y_train)\n","\n","# Create a mask\n","mask = rfe.support_\n","\n","# Apply the mask to the feature dataset X and print the result\n","reduced_X = X.loc[:, mask]\n","print(reduced_X.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting estimator with 8 features.\n","Fitting estimator with 6 features.\n","Fitting estimator with 4 features.\n","Index(['glucose', 'age'], dtype='object')\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ZxITJlmSSh9h","colab_type":"text"},"source":["¡Excelente! En comparación con el método de umbral rápido del ejercicio anterior, una de las características seleccionadas es diferente."]},{"cell_type":"markdown","metadata":{"id":"RRYcGm0fSh9i","colab_type":"text"},"source":["# Regresión lineal regularizada\n","\n","Ahora utilizaremos el concepto de regularización, para evitar el sobre ajuste del modelo de predicción e igualmente identificar las características suceptibles de ser seleccionadas."]},{"cell_type":"markdown","metadata":{"id":"psQ9CH1KSh9j","colab_type":"text"},"source":["# Crear un regresor LASSO\n","\n","Ahora se trabajará en el conjunto de datos numéricos de mediciones corporales ANSUR para predecir el índice de masa corporal (BMI) de una persona utilizando un regresor Lasso (Least Absolute Shrinkage and Selection Operator)  preimportado. El BMI  es una métrica derivada de la altura y el peso del cuerpo, pero esas dos características se han eliminado del conjunto de datos para forzar al modelo aun mayor análisis.\n","\n","Primero, estandarizará los datos utilizando el StandardScaler() que se ha instanciado como scaler para asegurarse de que todos los coeficientes se enfrentan a una fuerza de regularización comparable que intenta reducirlos.\n","\n","Todas las funciones y clases necesarias más los conjuntos de datos de entrada X y y cargarán a continuación."]},{"cell_type":"code","metadata":{"id":"G-WJWkGbSh9j","colab_type":"code","colab":{},"outputId":"cbf96b10-0e46-42c2-c9b0-5afb77561fff"},"source":["from sklearn.linear_model import Lasso\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","X=pd.read_csv(\"ANSUR-MALE.csv\", encoding='latin-1')#reading a dataset in a dataframe using pandas\n","y=pd.read_csv(\"y_ANSUR.csv\", encoding='latin-1')#reading a dataset in a dataframe using pandas\n","y =y['BMI']\n","X = X.drop(['Age','Branch','Component','DODRace','Date','Ethnicity','Gender','Heightin','Installation','PrimaryMOS','SubjectNumericRace','SubjectsBirthLocation','Weightlbs','WritingPreference','stature',\n","'subjectid','weightkg'], axis=1)\n","print(X.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4082, 91)\n","(4082,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vZSNvxw6Sh9o","colab_type":"text"},"source":["Establezca un conjunto de evaluación del 30%,  estableciendo una relación de conjuntos de entrenamiento-prueba 70% -30%."]},{"cell_type":"code","metadata":{"id":"ALRTjlt8Sh9p","colab_type":"code","colab":{}},"source":["# Set the test size to 30% to get a 70-30% train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=____, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBVGI49xSh9v","colab_type":"text"},"source":["Ajuste el escalador en las características de entrenamiento y transfórmelas de una vez."]},{"cell_type":"code","metadata":{"id":"N31_Zl0ASh9v","colab_type":"code","colab":{}},"source":["# Fit the scaler on the training features and transform these in one go\n","X_train_std = scaler.____"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmdotZmUSh90","colab_type":"text"},"source":["Cree un modelo LASSO"]},{"cell_type":"code","metadata":{"id":"pQl4VPazSh90","colab_type":"code","colab":{}},"source":["# Create the Lasso model\n","la = ____()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qj4pOyJDSh97","colab_type":"text"},"source":["Ajúste el modelo LASSO a los datos de entrenamiento escalados"]},{"cell_type":"code","metadata":{"id":"CHQ0Hm91Sh98","colab_type":"code","colab":{},"outputId":"a0ad4cb7-30e9-4384-d9c7-37ff0775fb7d"},"source":["# Fit it to the standardized training data\n","la.____"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n","      normalize=False, positive=False, precompute=False, random_state=None,\n","      selection='cyclic', tol=0.0001, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":245}]},{"cell_type":"markdown","metadata":{"id":"VphjO_V4Sh-B","colab_type":"text"},"source":["¡Buen trabajo! Has ajustado el modelo Lasso a los datos de entrenamiento estandarizados. ¡Ahora veamos los resultados!"]},{"cell_type":"markdown","metadata":{"id":"DwZ9t3EnSh-C","colab_type":"text"},"source":["# Resultados del modelo LASSO:\n","\n","Ahora que se ha entrenado el modelo LASSO, evaluaremos su capacidad predictiva ( R2 ) en el conjunto de prueba, y enumeremos cuantas características son ignoradas al ser sus coeficientes reducidos a cero."]},{"cell_type":"code","metadata":{"id":"EW-MHDehSh-D","colab_type":"code","colab":{},"outputId":"158f5cab-c1e9-4079-ccec-df5fe1f6befb"},"source":["# Transform the test set with the pre-fitted scaler\n","X_test_std = scaler.____\n","\n","# Calculate the coefficient of determination (R squared) on X_test_std\n","r_squared = la.____(____, ____)\n","print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n","\n","# Create a list that has True values when coefficients equal 0\n","zero_coef = la.____ == ____\n","\n","# Calculate how many features have a zero coefficient\n","n_ignored = sum(____)\n","print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model can predict 84.7% of the variance in the test set.\n","The model has ignored 82 out of 91 features.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vpyfX_XmSh-H","colab_type":"text"},"source":["¡Que bien! Podemos predecir casi el 85% de la varianza en el valor de IMC usando solo 9 de 91 de las características. Sin embargo, el R ^ 2 podría ser más alto."]},{"cell_type":"markdown","metadata":{"id":"jRBpd8XvSh-I","colab_type":"text"},"source":["# Ajustando la influencia de la regularización\n","\n","El modelo actual de LASSO tiene una puntuación (R2) del 84.7%. Cuando un modelo aplica una regularización demasiado grande, puede sufrir un alto sesgo, lo que perjudica su poder predictivo.\n","\n","Mejoremos el equilibrio entre el poder predictivo y la simplicidad del modelo ajustando el parámetro alpha."]},{"cell_type":"markdown","metadata":{"id":"33zdG4IDSh-J","colab_type":"text"},"source":["Encontrar el valor más alto para alpha que mantendrá  el valor (R2) por encima de 98% de las opciones: 1,   0.50,   0.1   y  0.01"]},{"cell_type":"code","metadata":{"id":"DExRDyCMSh-K","colab_type":"code","colab":{},"outputId":"aa539fa0-fbb0-4d71-dca6-686e9b2bf0bb"},"source":["# Find the highest alpha value with R-squared above 98%\n","la = Lasso(____, random_state=0)\n","\n","# Fits the model and calculates performance stats\n","la.fit(X_train_std, y_train)\n","r_squared = la.score(X_test_std, y_test)\n","n_ignored_features = sum(la.coef_ == 0)\n","\n","# Print peformance stats \n","print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n","print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model can predict 98.3% of the variance in the test set.\n","64 out of 91 features were ignored.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wPrRSCkISh-O","colab_type":"text"},"source":["¡Excelente! Con esta valor para la regularización, podemos predecir el 98% de la varianza en el valor de IMC mientras ignoramos 2/3 de las características."]},{"cell_type":"markdown","metadata":{"id":"1tuDAJ-9Sh-P","colab_type":"text"},"source":["# Combinando Selectores de Características\n","\n","Ahora encontraremos de manera automática el valor óptimo  de Alpha, mediante el uso de el regresor LassoCV."]},{"cell_type":"markdown","metadata":{"id":"OeabSN1ESh-Q","colab_type":"text"},"source":["# Creando un regresor LassoCV\n","\n","Ahora predeciremos la circunferencia del bíceps en una submuestra del conjunto de datos ANSUR masculino utilizando el regresor LassoCV()  que ajusta automáticamente la intensidad de la regularización (valor alfa) mediante la validación cruzada.\n","\n","Primero cargamos los datos. Y laslibrerias necesarias."]},{"cell_type":"code","metadata":{"id":"GpidOSVDSh-R","colab_type":"code","colab":{},"outputId":"d676cbc5-bc44-4216-a50e-c810e56e5f86"},"source":["X=pd.read_csv(\"ANSUR-MALE.csv\", encoding='latin-1')#reading a dataset in a dataframe using pandas\n","mask=['acromialheight','axillaheight','bideltoidbreadth','buttockcircumference','buttockkneelength','buttockpopliteallength','cervicaleheight','chestcircumference','chestheight','earprotrusion','footbreadthhorizontal','forearmcircumferenceflexed','handlength','headbreadth','heelbreadth','hipbreadth','iliocristaleheight','interscyeii','lateralfemoralepicondyleheight','lateralmalleolusheight','neckcircumferencebase','radialestylionlength','shouldercircumference','shoulderelbowlength','sleeveoutseam','thighcircumference','thighclearance','verticaltrunkcircumferenceusa','waistcircumference','waistdepth','wristheight']\n","X=X.loc[0:999, mask]\n","BMI=pd.read_csv(\"y_ANSUR.csv\", encoding='latin-1')#reading a dataset in a dataframe using pandas\n","mask=['BMI']\n","BMI=BMI.loc[0:999, mask]\n","X['BMI']=BMI\n","y=pd.read_csv(\"y-lassoCV.csv\", encoding='latin-1')#reading a dataset in a dataframe using pandas\n","y =y['Biceps']\n","print(XX.shape)\n","print(y.shape)\n","# Set the test size to 30% to get a 70-30% train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","X_train = scaler.fit_transform(X_train)\n","\n","X_test = scaler.transform(X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1000, 32)\n","(1000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EBUbJCyMSh-W","colab_type":"text"},"source":["Cree y ajuste el modelo LassoCV en el conjunto de entrenamiento."]},{"cell_type":"code","metadata":{"id":"gjH-fvu6Sh-X","colab_type":"code","colab":{},"outputId":"cd627b53-771e-4cc0-cb0e-6ec76959c81c"},"source":["from sklearn.linear_model import LassoCV\n","\n","# Create and fit the LassoCV model on the training set\n","lcv = ____\n","lcv.____\n","print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Optimal alpha = 0.089\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VvA90aGQSh-d","colab_type":"text"},"source":["Calculamos el (R2) en el conjunto de prueba"]},{"cell_type":"code","metadata":{"id":"8lcW5XB0Sh-e","colab_type":"code","colab":{},"outputId":"4a87ccb7-ae6d-4e72-edbd-c199e4c81182"},"source":["# Calculate R squared on the test set\n","r_squared = lcv.____\n","print('The model explains {0:.1%} of the test set variance'.format(r_squared))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model explains 88.2% of the test set variance\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_pe1G5yiSh-j","colab_type":"text"},"source":["Creamos una máscara para coeficientes que no sean iguales a cero"]},{"cell_type":"code","metadata":{"id":"wlsRiVc0Sh-k","colab_type":"code","colab":{},"outputId":"6459962b-56e5-4520-91b8-678338bb873c"},"source":["# Create a mask for coefficients not equal to zero\n","lcv_mask = ____\n","print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["26 features out of 32 selected\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7VyjUr5oSh-s","colab_type":"text"},"source":["¡Excelente! Obtuvimos una R2 bueno y eliminamos 6 características. Guardaremos la máscara lcv_mask para más adelante."]},{"cell_type":"markdown","metadata":{"id":"_uiYQZ33Sh-t","colab_type":"text"},"source":["# Ensamble de modelos y votación\n","\n","El LassoCV()modelo seleccionó 26 de las 32 características. No está mal, pero tampoco es una reducción espectacular de la dimensionalidad. Usemos dos modelos más para seleccionar las 10 características que consideran más importantes utilizando el Eliminador de funciones recursivas (RFE)."]},{"cell_type":"markdown","metadata":{"id":"zdiT43k3Sh-u","colab_type":"text"},"source":["Seleccione 10 funciones con RFE en un GradientBoostingRegressor, descarte 3 características en cada paso."]},{"cell_type":"code","metadata":{"id":"N_t-o1qoSh-v","colab_type":"code","colab":{},"outputId":"57567901-109d-45d6-c844-c4b01196146e"},"source":["from sklearn.feature_selection import RFE\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n","rfe_gb = RFE(estimator=____, \n","             n_features_to_select=____, step=____, verbose=1)\n","rfe_gb.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting estimator with 32 features.\n","Fitting estimator with 29 features.\n","Fitting estimator with 26 features.\n","Fitting estimator with 23 features.\n","Fitting estimator with 20 features.\n","Fitting estimator with 17 features.\n","Fitting estimator with 14 features.\n","Fitting estimator with 11 features.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["RFE(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse',\n","                                        init=None, learning_rate=0.1, loss='ls',\n","                                        max_depth=3, max_features=None,\n","                                        max_leaf_nodes=None,\n","                                        min_impurity_decrease=0.0,\n","                                        min_impurity_split=None,\n","                                        min_samples_leaf=1, min_samples_split=2,\n","                                        min_weight_fraction_leaf=0.0,\n","                                        n_estimators=100, n_iter_no_change=None,\n","                                        presort='auto', random_state=None,\n","                                        subsample=1.0, tol=0.0001,\n","                                        validation_fraction=0.1, verbose=0,\n","                                        warm_start=False),\n","    n_features_to_select=10, step=3, verbose=1)"]},"metadata":{"tags":[]},"execution_count":318}]},{"cell_type":"markdown","metadata":{"id":"yHYZGZwJSh-1","colab_type":"text"},"source":["Calculamos el R2 en el conjunto de pruebas"]},{"cell_type":"code","metadata":{"id":"BuSe8ZQCSh-2","colab_type":"code","colab":{},"outputId":"a404fdaf-ddf8-456c-9389-f1ef958af617"},"source":["# Calculate the R squared on the test set\n","r_squared = rfe_gb.____\n","print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model can explain 85.6% of the variance in the test set\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y05uF0TFSh--","colab_type":"text"},"source":["Asigne la matriz de soporte del modelo ajustado a gb_mask"]},{"cell_type":"code","metadata":{"id":"PO3YCBIOSh_A","colab_type":"code","colab":{}},"source":["# Assign the support array to gb_mask\n","gb_mask = ____"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gQ_W2CRSh_F","colab_type":"text"},"source":["Modifique ahora el primer paso para seleccionar 10 características con RFE en un RandomForestRegressor(), elimine 3 características en cada paso"]},{"cell_type":"code","metadata":{"id":"SCmbgVfASh_G","colab_type":"code","colab":{},"outputId":"47833d1f-3fc3-4049-fb49-b047fa6a886e"},"source":["# Modify the first step to select 10 features with RFE on a RandomForestRegressor() and drop 3 features on each step.\n","\n","from sklearn.feature_selection import RFE\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n","rfe_rf = RFE(estimator=____, \n","             n_features_to_select=____, step=____, verbose=1)\n","rfe_rf.fit(X_train, y_train)\n","\n","# Calculate the R squared on the test set\n","r_squared = rfe_rf.score(X_test, y_test)\n","print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n","\n","# Assign the support array to gb_mask\n","rf_mask = rfe_rf.support_"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting estimator with 32 features.\n","Fitting estimator with 29 features.\n","Fitting estimator with 26 features.\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting estimator with 23 features.\n","Fitting estimator with 20 features.\n","Fitting estimator with 17 features.\n","Fitting estimator with 14 features.\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting estimator with 11 features.\n","The model can explain 83.4% of the variance in the test set\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n","D:\\Users\\MOYCOT\\Anaconda3\\envs\\UNAL_CRP\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7aOQcClGSh_M","colab_type":"text"},"source":["¡Buen trabajo! Incluyendo el modelo lineal Lasso del ejercicio anterior, ahora tenemos los votos de 3 modelos en los que las características son importantes."]},{"cell_type":"markdown","metadata":{"id":"FWlKYuaVSh_M","colab_type":"text"},"source":["# Combinando 3 selectores de cartacterísticas\n","\n","Combinaremos los votos de los 3 modelos que construyó en los ejercicios anteriores, para decidir qué características son importantes en una máscara global. Luego usaremos esta máscara para reducir la dimensionalidad y ver cómo funciona un regresor lineal simple en el conjunto de datos reducido."]},{"cell_type":"markdown","metadata":{"id":"eL_-I3Q8Sh_N","colab_type":"text"},"source":["Sume los votos de los tres modelos usando np.sum()"]},{"cell_type":"code","metadata":{"id":"wCE6-H7VSh_O","colab_type":"code","colab":{},"outputId":"2169057f-1e4d-4429-d13b-f9fa1e0b7039"},"source":["# Sum the votes of the three models\n","votes = ____\n","print(votes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1 0 3 3 0 1 0 3 2 1 1 3 1 1 1 2 0 1 1 2 0 1 3 1 0 3 1 2 3 1 1 3]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LkbFbl55Sh_T","colab_type":"text"},"source":["Crear una máscara global para todas las características seleccionadas por los 3 modelos"]},{"cell_type":"code","metadata":{"id":"b8WNwydgSh_U","colab_type":"code","colab":{},"outputId":"3afd79f0-71b7-45cf-cfd3-ecd55f60ee40"},"source":["# Create a mask for features selected by all 3 models\n","meta_mask = ____\n","print(meta_mask)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[False False  True  True False False False  True False False False  True\n"," False False False False False False False False False False  True False\n"," False  True False False  True False False  True]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s2b96xkhSh_d","colab_type":"text"},"source":["Aplique reduccción de la dimesionalidad en X, e imprima que características fueron seleccionadas"]},{"cell_type":"code","metadata":{"id":"d1T_7zApSh_e","colab_type":"code","colab":{},"outputId":"d36b5db6-4409-4a02-fe03-da589b188429"},"source":["# Apply the dimensionality reduction on X\n","X_reduced = ____\n","print(X_reduced.columns)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference',\n","       'forearmcircumferenceflexed', 'shouldercircumference',\n","       'thighcircumference', 'waistcircumference', 'BMI'],\n","      dtype='object')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mf_eGIe5Sh_j","colab_type":"text"},"source":["Ingrese el conjunto reducido para realizar una regresión lineal simple"]},{"cell_type":"code","metadata":{"id":"vUAoL2u7Sh_k","colab_type":"code","colab":{},"outputId":"99b32d59-c3a9-4158-b896-51f660d2ca29"},"source":["from sklearn.linear_model import LinearRegression\n","# Plug the reduced dataset into a linear regression pipeline\n","X_train, X_test, y_train, y_test = train_test_split(____, y, test_size=0.3, random_state=0)\n","lm.fit(scaler.fit_transform(X_train), y_train)\n","r_squared = lm.score(scaler.transform(X_test), y_test)\n","print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model can explain 87.1% of the variance in the test set using 8 features.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6uk-6gx3Sh_p","colab_type":"text"},"source":["¡Perfecto! ¡Usando los votos obtenidos de 3 modelos, pudimos seleccionar  seleccionar solo 8 características que permitieron que un modelo lineal simple obtuviera una alta precisión!"]}]}